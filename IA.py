# -*- coding: utf-8 -*-
"""TF-Rocio-Jose-Doglas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wainDmtZHAGaC5rpqtoQmiUyuvUSlF-V

#**Trabajo Final - Antecedentes Causa Conductor Ciudad de Barcelona**

**Autores:**<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Rocio Ramos Echague<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;José Antonio Espinoza Franco<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Doglas A. Dembogurski Feix<br>
**Fecha:**&nbsp;11-10-2023

***

**#El origen de los siguientes datasets es :** https://datos.gob.es/es/catalogo/l01080193-accidentes-segun-causa-conductor-gestionados-por-la-guardia-urbana-a-la-ciutat-de-barcelona

2022
https://opendata-ajuntament.barcelona.cat/data/dataset/29d1b774-a83e-4c1e-91f7-1b9ad042ea83/resource/87a8aeda-d3eb-4ba5-bcad-b9ab0c296df5/download

2021
https://opendata-ajuntament.barcelona.cat/data/dataset/29d1b774-a83e-4c1e-91f7-1b9ad042ea83/resource/b384d295-5aec-49b1-86de-c3096ee39209/download

2020
https://opendata-ajuntament.barcelona.cat/data/dataset/29d1b774-a83e-4c1e-91f7-1b9ad042ea83/resource/ab22b7f9-a780-4c8b-8605-eb0e35b4094c/download

2019
https://opendata-ajuntament.barcelona.cat/data/dataset/29d1b774-a83e-4c1e-91f7-1b9ad042ea83/resource/240bbe6f-b01e-4c7c-8f5f-69ce317a1d01/download

<font color="black">**Los datasets se cargan desde google drive...**</font>

2022
https://drive.google.com/file/d/1NYiocpsSCaPzN22Tl_4yV4KzkbICu8xp/view?usp=sharing

2021
https://drive.google.com/file/d/1p9_fJRvXFygn_0ypClXiidVkB2HrQO3D/view?usp=sharing

2020
https://drive.google.com/file/d/1LNePR-U0DD8fBjg4bOhxm-BFFlMhMTWU/view?usp=sharing

2019
https://drive.google.com/file/d/1_mEp_r5UkpLCdfifw8L-2WjDsLpJLsYQ/view?usp=sharing

<font color="green">*#Importar librerías...*</font><br>
<font color="green">*#Librerias para manejo de datos...*</font><br>
https://pandas.pydata.org/docs/index.html<br>
https://numpy.org/<br>
<font color="green">*#Para graficos...*</font><br>
https://matplotlib.org/
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""**Esto hay que sacar**

<font color="green">*#Carga de datos del año 2022*</font><br><br>
<font color="green">*#Abrir el dataset como un dataframe:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html<br>
<font color="green">*#Leer los datos:</font> https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html<br>
<font color="green">*#Visualizar datos:*</font>
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html

Entrada: Identificador del archivo.

Salida: Una tabla DataFrame de Pandas.
"""

file_id_2022 = '1NYiocpsSCaPzN22Tl_4yV4KzkbICu8xp'
url_2022 = f'https://drive.google.com/uc?id={file_id_2022}&export=download'
dtf_2022 = pd.read_csv(url_2022)
dtf_2022.head()

"""<font color="green">*#Carga de datos del año 2021*</font><br><br>
<font color="green">*#Abrir el dataset como un dataframe:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html<br>
<font color="green">*#Leer los datos:</font> https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html<br>
<font color="green">*#Visualizar datos:*</font>
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html<br>
Entrada: Identificador del archivo.

Salida: Una tabla DataFrame de Pandas.
"""

file_id_2021 = '1p9_fJRvXFygn_0ypClXiidVkB2HrQO3D'
url_2021 = f'https://drive.google.com/uc?id={file_id_2021}&export=download'
dtf_2021 = pd.read_csv(url_2021)
dtf_2021.head()

"""<font color="green">*#Carga de datos del año 2020*</font><br><br>
<font color="green">*#Abrir el dataset como un dataframe:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html<br>
<font color="green">*#Leer los datos:</font> https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html<br>
<font color="green">*#Visualizar datos:*</font>
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html<br>
Entrada: Identificador del archivo.

Salida: Una tabla DataFrame de Pandas.
"""

file_id_2020 = '1LNePR-U0DD8fBjg4bOhxm-BFFlMhMTWU'
url_2020 = f'https://drive.google.com/uc?id={file_id_2020}&export=download'
dtf_2020 = pd.read_csv(url_2020)
dtf_2020.head()

"""<font color="red">**Nota importante:**</font>  <font color="red">Se debe renombrar 2 columnas porque el orden de las cabeceras no es correcto, **Descripcio_torn** por **Descripcio_causa_mediata** y Cambiar el nombre de **Descripcio_causa_conductor** por **Descripcio_torn**</font>

<font color="green">*#Renombrar columnas:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html<br>
<font color="green">*#Visualizar datos:*</font>
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html<br>

Entrada: dataframe con las  columnas Descripcio_torn y Descripcio_causa_conductor.

Salida: El mismo dataframe, pero con las columnas renombradas.
"""

#Se procede a realizar los cambios de nombres
dtf_2020 = dtf_2020.rename(columns={"Descripcio_torn": "Descripcio_causa_mediata", "Descripcio_causa_conductor": "Descripcio_torn"})
dtf_2020.head()
# El resultado es que las cabeceras del Dataframe 2020 coinciden con los demas cargados 2022,2021

"""<font color="green">*#Carga de datos del año 2019*</font><br><br>
<font color="green">*#Abrir el dataset como un dataframe:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html<br>
<font color="green">*#Leer los datos:</font> https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html<br>
<font color="green">*#Visualizar datos:*</font>
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html<br>
<font color="green">*#Renombrar columnas:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html<br>
Entrada: Identificador del archivo.

Salida: Una tabla DataFrame de Pandas.
"""

file_id_2019 = '1_mEp_r5UkpLCdfifw8L-2WjDsLpJLsYQ'
url_2019 = f'https://drive.google.com/uc?id={file_id_2019}&export=download'
dtf_2019 = pd.read_csv(url_2019)
dtf_2019.head()

# Renombrar las columnas
dtf_2019 = dtf_2019.rename(columns={"Descripcio_torn": "Descripcio_causa_mediata", "Descripcio_causa_conductor": "Descripcio_torn"})
dtf_2019.head()

""" <font color="green">*#Cargar todos los dataframes en un array para facilitar la union en el siguiente paso*</font>


 Entrada: dataframes correspondientes a datos historicos.

Salida: Union de todos los dataframes.
"""

dataframes = [dtf_2022,dtf_2021,dtf_2020,dtf_2019]

""" <font color="green">*#Ver el tamaño de cada conjunto de datos para corroborar mas adelante despues de la union:*</font>  https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html

 Entrada: dataframe correspondientes a los años selecionados.

Salida: Tamaño fila-columna de cada dataframe.
"""

print('--2022--')
print(dtf_2022.shape)
print('--2021--')
print(dtf_2021.shape)
print('--2020--')
print(dtf_2020.shape)
print('--2019--')
print(dtf_2019.shape)

"""<font color="green">*#Unir los dataframes para tener una sola fuente de datos:*</font> https://pandas.pydata.org/docs/reference/api/pandas.concat.html<br>
Entrada: Arreglo de Pandas DataFrame

Salida: Un solo Pandas DataFrame
"""

#El comando pd.concat() se utiliza para combinar, fusionar datafremes
merged_dtf = pd.concat(dataframes,axis=0)
#El resultado es un solo dataframe combinado

# Visualizar las primeras filas del resultado
merged_dtf.head()
# Imprimir la cantidad de filas y columnas del resultado
print(merged_dtf.shape)

"""# **Analisis exploratorio de datos**

<font color="green">*#Visualizar las primeras filas del conjunto de datos:*</font>  https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html
"""

merged_dtf.head()

"""<font color="green">*#Ver tamaño del conjunto de datos:*</font> https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html"""

print(merged_dtf.shape)

"""<font color="green">*#Ver los nombres de los atributos a través de la lista retornada por .columns:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html

Entrada: Union de dataframes.

Salida: Total Columnas de dicha union.
"""

print(merged_dtf.columns)

"""<font color="green">*#Ver información general:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html

Entrada: La union de los dataframes.

Salida: Informacion correspondiente del dataframe.
"""

print(merged_dtf.info())

"""<font color="green">*#Ver las estadísticas de los atributos:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html

Salida: Distribución estadistica correspondiente al dataframe.
"""

merged_dtf.describe()

"""<font color="green">*#Ver estadísticas de atributos categóricos:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html

Entrada: El dataframe con columnas con el tipo de dato object y booleano.

Salida: Estadisiticas descriptiva correspondiente a las columnas.

"""

merged_dtf.describe(include=["object", "bool"])

"""<font color="green">*#Visualizar valores nulos:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html

Salida: Descripición estadistica de los valores nulos.
"""

null_merged_dtf=merged_dtf.isna()
null_merged_dtf.describe()

"""<font color="green">*#Visualizar datos:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html

Salida: Muestra Las primeras cinco columnas del dataframe indicando como verdadero o falso los datos nulos.
"""

null_merged_dtf.head()

"""
<font color="green">*#Importamos librerias pyplot para graficos:*</font>https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html

"""

import matplotlib.pyplot as plt

"""<font color="green">*#Obtener la distribución de las clases*</font>
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.value_counts.html

Entrada: columna Descripcio_causa_mediata del dataframe.

Salida: Cantidad total de clases con respecto a Descripcio_causa_mediata en el conjunto del dataframe.
"""

classes = merged_dtf["Descripcio_causa_mediata"].value_counts()

"""<font color="green">*#Visualizar la distribucion de la variable a predecir*</font><br>
https://matplotlib.org/3.5.3/api/figure_api.html<br>
https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.xticks.html<br>
https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.bar.html<br>
https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.xlabel.html<br>
https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.ylabel.html<br>
https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.show.html


"""

# Visualizar la distribución de las clases

#tamaño dl histograma
plt.figure(figsize=(12, 8))

#rotacion de 90 grados del titulo de cada clase
plt.xticks(rotation = 90)

#utilizacion de los parametros classes.index, classes.values para las barras correspondientes
plt.bar(classes.index, classes.values)

plt.xlabel("Descripcio_causa_mediata")
plt.ylabel("Número de accidentes")
plt.show()

"""<font color="green">*#Visualizar y comprobar el desequilibrio de las clases para su posterior preprocesamiento *</font>

Entrada: La columna Descripcio_causa_mediata del dataframe y como segundo parametro sort que indica el orden de las clases de mayor a menor.

Salida: Clases en orden decreciente.
"""

print(pd.value_counts(merged_dtf['Descripcio_causa_mediata'], sort = True))
#Vemos que existe un desbalanceo y una gran cantidad mayoritaria para la clase  de Manca d'atenció a la conducció , esto implica que es muy prpbable que tienda clasificar mayormente los accidentes como Manca d'atenció a la conducció.

"""# **Preprocesamiento**

<font color="maroon">**Limpieza de datos**</font>

<font color="green">*#Eliminar Atributos a no ser utilizados:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html<br>
<font color="green">*#Visualizar datos:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html

Entrada: Etiquetas no predictoras.

Salida: Dataframe con los atributos a predecir.
"""

atributos_eliminar = ['Numero_expedient','Codi_districte','Codi_barri','Nom_barri','Codi_carrer','Nom_carrer','Num_postal ','Mes_any','Dia_mes','Coordenada_UTM_X_ED50','Coordenada_UTM_Y_ED50','Longitud','Latitud']

#creamos un dataframe auxiliar
copia_dtf = merged_dtf.drop(atributos_eliminar, axis=1)

copia_dtf.head()

"""<font color="green">*#Verificar instancias con valores perdidos:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html

Salida: Contabilizacion de valores nulos correspondiente a cada columna del dataframe.
"""

print(copia_dtf.isna().sum())
copia_dtf.isna()

"""<font color="orangered">*#Se corrobora que no se encuentran datos perdidos!*</font>

<font color="green">*#Analizar los atributos del Dataframe Limpio:*</font> https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html

Salida: Informacion detallada del dataframe.
"""

copia_dtf.info()

"""<font color="green">*#Aplicamos normalización de atributos a la columna NK_Any<br>Visualizar los datos:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html

Entrada: DataFrame auxiliar que contiene una columna llamada NK_Any.

Salida: Descripcion estadisitca de la normalizacion de la columna NK_Any.
"""

NK_Any_dtf=   (copia_dtf['NK_Any']-copia_dtf['NK_Any'].min())  /  (copia_dtf['NK_Any'].max()-copia_dtf['NK_Any'].min())
NK_Any_dtf.describe()

"""<font color="green">*#Remplazamos la columna con los atributos normalizados*</font><br>
<font color="green">*#Visualizar datos:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html


Salida: Dataframe con la normalizacion aplicada a NK_Any
"""

copia_dtf['NK_Any']=NK_Any_dtf
copia_dtf.head()

"""<font color="green">*#Aplicamos normalización de atributos a la columna Hora_dia*</font>

Entrada: DataFrame auxiliar que contiene una columna llamada Hora_dia.

Salida: Descripcion estadisitca de la normalizacion de la columna Hora_dia.
"""

Hora_dia_dtf=   (copia_dtf['Hora_dia']-copia_dtf['Hora_dia'].min())  /  (copia_dtf['Hora_dia'].max()-copia_dtf['Hora_dia'].min())
Hora_dia_dtf.describe()

"""<font color="green">*#Remplazamos la columna con los atributos normalizados*</font><br>
<font color="green">*#Visualizar datos:*</font>https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html

Salida: Dataframe con la normalizacion aplicada a Hora_dia
"""

copia_dtf['Hora_dia']=Hora_dia_dtf
copia_dtf.head()

"""<font color="green">*#Verificar los valores únicos de la columnas Object:*</font>https://pandas.pydata.org/docs/reference/api/pandas.unique.html"""

unique_values_Nom_districte = copia_dtf['Nom_districte'].unique()
print(unique_values_Nom_districte)
print("------------------------------------------------------------")
unique_values_Descripcio_dia_setmana = copia_dtf['Descripcio_dia_setmana'].unique()
print(unique_values_Descripcio_dia_setmana)
print("------------------------------------------------------------")
unique_values_Nom_mes = copia_dtf['Nom_mes'].unique()
print(unique_values_Nom_mes)
print("------------------------------------------------------------")
unique_values_Descripcio_causa_mediata = copia_dtf['Descripcio_causa_mediata'].unique()
print(unique_values_Descripcio_causa_mediata)
print("------------------------------------------------------------")
unique_values_Descripcio_torn = copia_dtf['Descripcio_torn'].unique()
print(unique_values_Descripcio_torn)
print("------------------------------------------------------------")

"""<font color="green">*#Aplicar codificacion  ingenua y One Hot para las columnas del tipo categorico (Object)*</font>

<font color="green">*#Aplicar codificacion Ingenua*</font>
"""

copia_dtf['Nom_districte'] = copia_dtf['Nom_districte'].astype('category')
copia_dtf['Nom_districte_category'] = copia_dtf['Nom_districte'].cat.codes

copia_dtf['Nom_mes'] = copia_dtf['Nom_mes'].astype('category')
copia_dtf['Nom_mes_category'] = copia_dtf['Nom_mes'].cat.codes

copia_dtf['Descripcio_dia_setmana'] = copia_dtf['Descripcio_dia_setmana'].astype('category')
copia_dtf['Descripcio_dia_setmana_category'] = copia_dtf['Descripcio_dia_setmana'].cat.codes

copia_dtf['Descripcio_torn'] = copia_dtf['Descripcio_torn'].astype('category')
copia_dtf['Descripcio_torn_category'] = copia_dtf['Descripcio_torn'].cat.codes



copia_dtf.head()

"""<font color="green">*#Transformacion de datos</font>

<font color="green">*#Aplicar codificacion One hot*</font>

<font color="green">*#Crear instancia de One-hot-encoder y creamos una columna codificada*</font><br>https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
"""

#importamos la libreria de OneHotEncoder para la codificacion
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder()

#Aqui codificamos las columnas y lo convertimos en un arreglo

encoded_Nom_districte = pd.DataFrame(encoder.fit_transform(copia_dtf[['Nom_districte']]).toarray())
encoded_Nom_mes = pd.DataFrame(encoder.fit_transform(copia_dtf[['Nom_mes']]).toarray())
encoded_Descripcio_dia_setmana = pd.DataFrame(encoder.fit_transform(copia_dtf[['Descripcio_dia_setmana']]).toarray())
encoded_Descripcio_torn = pd.DataFrame(encoder.fit_transform(copia_dtf[['Descripcio_torn']]).toarray())

copia_dtf.head()

"""<font color="green">*#Renombrar columnas para evitar conflictos</font>"""

#Explicacion General: Renombramos cada columna evitando la sobreescritura.

for col in encoded_Nom_districte.columns:
    encoded_Nom_districte = encoded_Nom_districte.rename(columns={col: 'Nom_districte_' + str(col)})

for col in encoded_Nom_mes.columns:
    encoded_Nom_mes = encoded_Nom_mes.rename(columns={col: 'Nom_mes_' + str(col)})

for col in encoded_Descripcio_dia_setmana.columns:
    encoded_Descripcio_dia_setmana = encoded_Descripcio_dia_setmana.rename(columns={col: 'Desc_dia_set_' + str(col)})

for col in encoded_Descripcio_torn.columns:
    encoded_Descripcio_torn = encoded_Descripcio_torn.rename(columns={col: 'Desc_torn_' + str(col)})

"""<font color="green">*#Unir las columnas one-hot el dataframe</font><br>
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html
"""

#realizamos una copia del dataframe
Copia_auxiliar_dtfp = copia_dtf.copy()


#Unimos las codidicaciones ingenuas en el dataframe auxiliar
Copia_auxiliar_dtfp = Copia_auxiliar_dtfp.join(encoded_Nom_districte)
Copia_auxiliar_dtfp = Copia_auxiliar_dtfp.join(encoded_Nom_mes)
Copia_auxiliar_dtfp = Copia_auxiliar_dtfp.join(encoded_Descripcio_dia_setmana)
Copia_auxiliar_dtfp = Copia_auxiliar_dtfp.join(encoded_Descripcio_torn)

"""<font color="green">*#Eliminar las variables redundantes</font><br>
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html
"""

atributos_eliminar = ['Nom_districte','Descripcio_dia_setmana','Nom_mes','Descripcio_torn','Nom_districte_category','Descripcio_dia_setmana_category','Nom_mes_category','Descripcio_torn_category']
Copia_auxiliar_dtfp = Copia_auxiliar_dtfp.drop(atributos_eliminar, axis=1)
Copia_auxiliar_dtfp.head()

"""<font color="green">*#--------------Aplicar la estrategia de Oversampling de la clase minoritaria para equilibrar las clases desbalanceados -----------------------------*</font><br>
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.value_counts.html
"""

print(pd.value_counts(Copia_auxiliar_dtfp['Descripcio_causa_mediata'], sort = True))
#vemos que existe un desbalanceo y una gran cantidad mayoritaria para la clase  de Manca d'atenció a la conducció , esto implica que es muy prpbable que tienda clasificar mayormente los accidentes como Manca d'atenció a la conducció

"""<font color="green">*#Importar librerias para Over Sampling:<br></font>
https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html

<font color="green">*#Importar la libreria collections para ver las distribuiciones de las clases:<br></font>

https://docs.python.org/3/library/collections.html
"""

# Importa las bibliotecas necesarias
import pandas as pd
from imblearn.over_sampling import RandomOverSampler
from collections import Counter

X = Copia_auxiliar_dtfp.drop('Descripcio_causa_mediata', axis=1)
y = Copia_auxiliar_dtfp['Descripcio_causa_mediata']


objeto_ROS = RandomOverSampler(random_state=42)

# Aplicar oversampling a tus datos
X_resampled, y_resampled = objeto_ROS.fit_resample(X, y)

# Se crea un nuevo DataFrame con los datos resampleados para utilizar en los 2 modelos posteriores
df_resampled = pd.DataFrame(X_resampled, columns=X.columns)
df_resampled['Descripcio_causa_mediata'] = y_resampled

# Visualizar la distribución de las clases antes y despues del oversampling
print("Distribución de clases antes del oversampling:", Counter(y))
print("Distribución de clases después del oversampling:", Counter(y_resampled))

#Visulizar los datos resampleados
print(Copia_auxiliar_dtfp)

"""# **Organización de datos**

<font color="green">*#Aplicar partición de entrenamiento 70% y prueba 30%.</font><br>

https://scikit-learn.org/stable/model_selection.html<br>
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
"""

#Importamos esta libreria para dividir el conjunto de entrenamiento y test
from sklearn import model_selection
from sklearn.model_selection import train_test_split

"""<font color="green">*#Dividir los atributos, 70% para entrenamiento y 30% para test </font><br>"""

X_train, X_test, y_train, y_test = train_test_split(df_resampled.drop('Descripcio_causa_mediata', axis=1),
                                                    df_resampled['Descripcio_causa_mediata'],
                                                    test_size=0.3,
                                                    random_state=7)

"""# Aplicación de modelos

## Modelo 1: Support Vector Machine

Entrenamos un modelo SVM y predecimos etiquetas<br>
https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html<br>
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html



Salida: El resultado del modelo entrenado.
"""

#Importamos la libreria svm para el entrenamiento
from sklearn import svm

#la libreria classification_report nos provee las metricas de Precisión,Recall y F1-score que utilizaremos para la evaluacion.
from sklearn.metrics import classification_report

#Usamos kernel linear porque simple y eficiente además su complejidad computacional baja ya que utilizamos datos historicos
#Con el uso de class_weight = “balanced”  el modelo se encargará de equilibrar a la clase minoritaria, evitando que no se sesgue hacia la clase mayoritaria.
clf = svm.SVC(kernel='linear', class_weight="balanced")

clf.fit(X_train, y_train.values.ravel())

"""<font color="green">*#Realizamos la predicción del modelo SVM </font>


"""

y_pred_1 = clf.predict(X_test)

"""## EVALUACIÓN MODELO 1

<font color="green">*#Obtenemos una tabla de métricas para el SVM</font>

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html
"""

#print(classification_report(y_test,y_pred))
reporte_modelo_1 = classification_report(y_test, y_pred_1, zero_division=0)

print(reporte_modelo_1)

"""<font color="green">*#Importamos librerias para graficar</font><br>
https://matplotlib.org/stable/tutorials/pyplot.html<br>
<font color="green">*#Importamos libreria para manejo de expresiones regulares</font><br>https://docs.python.org/3/library/re.html
"""

import matplotlib.pyplot as plt
import re

"""<font color="green">*# Extraemos los valores de precisión para graficar y comparar los modelos*</font>"""

matches = re.findall(r'(\w+[^ \n]+)\s+(\d+\.\d+)\s+\d+\.\d+\s+\d+\.\d+\s+\d+', reporte_modelo_1)

# Separar los resultados en listas de clases y valores de precisión
class_names, precision_values = zip(*matches)

# Convertir los valores a tipo float
precision_values = [float(value) for value in precision_values]

# Creaamos el histograma con las correspondientes clases
plt.figure(figsize=(12, 6))
plt.bar(class_names, precision_values, color='blue')
plt.xlabel('Clases')
plt.ylabel('Precision')
plt.title('Precision por Clase')
plt.xticks(rotation=60, ha='right')
plt.ylim(0, 1)
plt.tight_layout()

# Mostramos el histograma
plt.show()

"""##Modelo 2: Random Forest

<font color="green">*#Entrenamos el modelo y predecimos etiquetas</font><br>
https://scikit-learn.org/stable/modules/ensemble.html<br>
https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
"""

#Importamos la libreria RandomForestClassifier para el entrenamiento
from sklearn.ensemble import RandomForestClassifier

#La libreria classification_report nos provee las metricas de Precisión,Recall y F1-score que utilizaremos para la evaluacion.
from sklearn.metrics import classification_report

#Con el uso de class_weight = “balanced”  el modelo se encargará de equilibrar a la clase minoritaria, evitando que no se sesgue hacia la clase mayoritaria.
rfc = RandomForestClassifier(class_weight="balanced")
rfc.fit(X_train, y_train.values.ravel())

"""<font color="green">*#Realizamos la predicción</font>"""

y_pred_2 = rfc.predict(X_test)

"""## EVALUACIÓN MODELO 2

<font color="green">*#Obtenemos una tabla de métricas para el Random Forest</font> <br>

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html
"""

reporte_modelo_2 = classification_report(y_test,y_pred_2)
print(reporte_modelo_2)

"""<font color="green">*# Extraemos los valores de precisión del segundo modelo para graficar y comparar con el grafico del modelo anterior*</font>"""

# Extraemos los valores de precisión y los nombres de las clases
matches = re.findall(r'(\w+[^ \n]+)\s+(\d+\.\d+)\s+\d+\.\d+\s+\d+\.\d+\s+\d+', reporte_modelo_2)

# Separar los resultados en listas de clases y valores de precisión
class_names, precision_values = zip(*matches)

# Convertir los valores a tipo float
precision_values = [float(value) for value in precision_values]

# Crear el histograma
plt.figure(figsize=(12, 6))
plt.bar(class_names, precision_values, color='blue')
plt.xlabel('Clases')
plt.ylabel('Precision')
plt.title('Precision por Clase')
plt.xticks(rotation=60, ha='right')
plt.tight_layout()

# Mostrar el histograma
plt.show()

"""##Discusión

<font color="maroon">**De los modelos seleccionados el Random Forest presenta un rendimiento muy bueno para los problemas de clasificación multiclase, en cambio SVM es muy adaptable a los datos de alta dimensionalidad como es el caso.**</font><br><br>
Al desarrollar el proyecto se pudo detectar los siguientes inconvenientes,  
los datos sin balancear arrojaban errores en los modelos como: Division by zero, los resultados se inclinaban hacia la clase mayoritaria <font color="black">"*Manca d'atenció a la conducció*"</font>, esto al balancear modelos pudieron ser ejecutados sin problemas y arrojaron mejores resultados pero no los optimos.
"""